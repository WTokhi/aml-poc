{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train a diabetes Model with AML pipeline\n",
        "\n",
        "\n",
        "## Content\n",
        "### 1. Connect to Workspace\n",
        "\n",
        "### 2. Create, upload and register diabetes dataset\n",
        "\n",
        "### 3. Setup an Azure Ml pipeline\n",
        "- Create scripts for pipeline steps\n",
        "- Prepare a compute environment \n",
        "- Define Python environment\n",
        "- Run pipeline as an experiment\n",
        "\n",
        "### 4. Deploy model for inference \n",
        "\n",
        "    "
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Connect to your workspace\n",
        "\n",
        "To get started, connect to your workspace.\n",
        "\n",
        ">**Note**: If you haven't already established an authenticated session with your Azure subscription, you'll be prompted to authenticate by clicking a link, entering an authentication code, and signing into Azure."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "# Load the workspace from the saved config file\n",
        "ws = Workspace.from_config()\n",
        "print('Ready to work with', ws.name)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ready to work with isolation_forest\n"
          ]
        }
      ],
      "execution_count": 15,
      "metadata": {}
    },
    {
      "source": [
        "# 2. Upload, create and register diabetes dataset"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading an estimated of 1 files\n",
            "Uploading ./data/diabetes.csv\n",
            "Uploaded ./data/diabetes.csv, 1 files out of an estimated total of 1\n",
            "Uploaded 1 files\n",
            "Creating dataset...\n",
            "Registering dataset...\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Dataset\n",
        "\n",
        "# Upload data files to the default datastore\n",
        "default_ds = ws.get_default_datastore()\n",
        "default_ds.upload_files(files=['./data/diabetes.csv'],\n",
        "                       target_path='diabetes-data/',\n",
        "                       overwrite=True,\n",
        "                       show_progress=True)\n",
        "\n",
        "#Create a tabular dataset from the path on the datastore\n",
        "print('Creating dataset...')\n",
        "data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
        "\n",
        "# Register the tabular dataset\n",
        "print('Registering dataset...')\n",
        "try:\n",
        "    data_set = data_set.register(workspace=ws, \n",
        "                               name='diabetes dataset',\n",
        "                               description='diabetes data',\n",
        "                               tags = {'format':'CSV'},\n",
        "                               create_new_version=True)\n",
        "except Exception as ex:\n",
        "    print(ex)\n"
      ]
    },
    {
      "source": [
        "# 3. Setup an Azure Ml pipeline\n",
        "## i. Create scripts for pipeline steps\n",
        "Pipelines consist of one or more steps, which can be Python scripts, or specialized steps like a data transfer step that copies data from one location to another. Each step can run in its own compute context. In this exercise, you'll build a simple pipeline that contains two Python script steps: one to pre-process some training data, and another to use the pre-processed data to train and register a model.\n",
        "\n",
        "First, let's create a folder for the script files we'll use in the pipeline steps."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diabetes_pipeline\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Create a folder for the pipeline step files\n",
        "experiment_folder = 'diabetes_pipeline'\n",
        "os.makedirs(experiment_folder, exist_ok=True)\n",
        "\n",
        "print(experiment_folder)"
      ]
    },
    {
      "source": [
        "Now let's create the first script, which will read data from the diabetes dataset and apply some simple pre-processing to remove any rows with missing data and normalize the numeric features so they're on a similar scale.\n",
        "\n",
        "The script includes a argument named --prepped-data, which references the folder where the resulting data should be saved."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting diabetes_pipeline/prep_diabetes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $experiment_folder/prep_diabetes.py \n",
        "# Import libraries\n",
        "import os\n",
        "import argparse\n",
        "import pandas as pd\n",
        "from azureml.core import Run\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Get parameters\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--input-data\", type=str, dest='raw_dataset_id', help='raw dataset')\n",
        "parser.add_argument('--prepped-data', type=str, dest='prepped_data', default='prepped_data', help='Folder for results')\n",
        "args = parser.parse_args()\n",
        "save_folder = args.prepped_data\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# load the data (passed as an input dataset)\n",
        "print(\"Loading Data...\")\n",
        "diabetes = run.input_datasets['raw_data'].to_pandas_dataframe()\n",
        "\n",
        "# Log raw row count\n",
        "row_count = (len(diabetes))\n",
        "run.log('raw_rows', row_count)\n",
        "\n",
        "# remove nulls\n",
        "diabetes = diabetes.dropna()\n",
        "\n",
        "# Normalize the numeric columns\n",
        "scaler = MinMaxScaler()\n",
        "num_cols = ['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree']\n",
        "diabetes[num_cols] = scaler.fit_transform(diabetes[num_cols])\n",
        "\n",
        "# Log processed rows\n",
        "row_count = (len(diabetes))\n",
        "run.log('processed_rows', row_count)\n",
        "\n",
        "# Save the prepped data\n",
        "print(\"Saving Data...\")\n",
        "os.makedirs(save_folder, exist_ok=True)\n",
        "save_path = os.path.join(save_folder,'data.csv')\n",
        "diabetes.to_csv(save_path, index=False, header=True)\n",
        "\n",
        "# End the run\n",
        "run.complete()"
      ]
    },
    {
      "source": [
        "Now you can create the script for the second step, which will train a model. The script includes a argument named --training-folder, which references the folder where the prepared data was saved by the previous step."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing diabetes_pipeline/train_diabetes.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile $experiment_folder/train_diabetes.py\n",
        "# Import libraries\n",
        "from azureml.core import Run, Model\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get parameters\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--training-folder\", type=str, dest='training_folder', help='training data folder')\n",
        "args = parser.parse_args()\n",
        "training_folder = args.training_folder\n",
        "\n",
        "# Get the experiment run context\n",
        "run = Run.get_context()\n",
        "\n",
        "# load the prepared data file in the training folder\n",
        "print(\"Loading Data...\")\n",
        "file_path = os.path.join(training_folder,'data.csv')\n",
        "diabetes = pd.read_csv(file_path)\n",
        "\n",
        "# Separate features and labels\n",
        "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
        "\n",
        "# Split data into training set and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
        "\n",
        "# Train adecision tree model\n",
        "print('Training a decision tree model...')\n",
        "model = DecisionTreeClassifier().fit(X_train, y_train)\n",
        "\n",
        "# calculate accuracy\n",
        "y_hat = model.predict(X_test)\n",
        "acc = np.average(y_hat == y_test)\n",
        "print('Accuracy:', acc)\n",
        "run.log('Accuracy', np.float(acc))\n",
        "\n",
        "# calculate AUC\n",
        "y_scores = model.predict_proba(X_test)\n",
        "auc = roc_auc_score(y_test,y_scores[:,1])\n",
        "print('AUC: ' + str(auc))\n",
        "run.log('AUC', np.float(auc))\n",
        "\n",
        "# plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_scores[:,1])\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "# Plot the diagonal 50% line\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "# Plot the FPR and TPR achieved by our model\n",
        "plt.plot(fpr, tpr)\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "run.log_image(name = \"ROC\", plot = fig)\n",
        "plt.show()\n",
        "\n",
        "# Save the trained model in the outputs folder\n",
        "print(\"Saving model...\")\n",
        "os.makedirs('outputs', exist_ok=True)\n",
        "model_file = os.path.join('outputs', 'diabetes_model.pkl')\n",
        "joblib.dump(value=model, filename=model_file)\n",
        "\n",
        "# Register the model\n",
        "print('Registering model...')\n",
        "Model.register(workspace=run.experiment.workspace,\n",
        "               model_path = model_file,\n",
        "               model_name = 'diabetes_model',\n",
        "               tags={'Training context':'Pipeline'},\n",
        "               properties={'AUC': np.float(auc), 'Accuracy': np.float(acc)})\n",
        "\n",
        "\n",
        "run.complete()"
      ]
    },
    {
      "source": [
        "## b. Prepare a compute environment for the pipeline steps\n",
        "In this exercise, you'll use the same compute for both steps, but it's important to realize that each step is run independently; so you could specify different compute contexts for each step if appropriate.\n",
        "\n",
        "First, get the compute target you created in a previous lab (if it doesn't exist, it will be created).\n",
        "\n",
        "Important: Change your-compute-cluster to the name of your compute cluster in the code below before running it! Cluster names must be globally unique names between 2 to 16 characters in length. Valid characters are letters, digits, and the - character."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing cluster, use it.\n"
          ]
        }
      ],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "cluster_name = \"regression\"\n",
        "\n",
        "try:\n",
        "    # Check for existing compute target\n",
        "    pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # If it doesn't already exist, create it\n",
        "    try:\n",
        "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', \n",
        "                                                               max_nodes=2,                                                                                                    vm_priority='dedicated')\n",
        "        pipeline_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
        "        pipeline_cluster.wait_for_completion(show_output=True)\n",
        "    except Exception as ex:\n",
        "        print(ex)"
      ]
    },
    {
      "source": [
        "## c. Define Python environment\n",
        "\n",
        "The compute will require a Python environment with the necessary package dependencies installed, so you'll need to create a run configuration."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n",
            "Run configuration created.\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Environment\n",
        "from azureml.core.conda_dependencies import CondaDependencies\n",
        "from azureml.core.runconfig import RunConfiguration\n",
        "\n",
        "# Create a Python environment for the experiment\n",
        "diabetes_env = Environment(\"diabetes-pipeline-env\")\n",
        "diabetes_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
        "diabetes_env.docker.enabled = True # Use a docker container\n",
        "\n",
        "# Create a set of package dependencies with versions \n",
        "diabetes_packages = CondaDependencies.create(\n",
        "    conda_packages=['scikit-learn','ipykernel','matplotlib','pandas','pip'],\n",
        "    pip_packages=['azureml-defaults','azureml-dataprep[pandas]','pyarrow']\n",
        "    )\n",
        "\n",
        "# Add the dependencies to the environment\n",
        "diabetes_env.python.conda_dependencies = diabetes_packages\n",
        "\n",
        "# Register the environment \n",
        "diabetes_env.register(workspace=ws)\n",
        "registered_env = Environment.get(ws, 'diabetes-pipeline-env')\n",
        "\n",
        "# Create a new runconfig object for the pipeline\n",
        "pipeline_run_config = RunConfiguration()\n",
        "\n",
        "# Use the compute you created above. \n",
        "pipeline_run_config.target = pipeline_cluster\n",
        "\n",
        "# Assign the environment to the run configuration\n",
        "pipeline_run_config.environment = registered_env\n",
        "\n",
        "print (\"Run configuration created.\")"
      ]
    },
    {
      "source": [
        "## d. Define pipeline steps and submit experiment\n",
        "\n",
        "Create and run a pipeline\n",
        "\n",
        "Now you're ready to create and run a pipeline.\n",
        "\n",
        "First you need to define the steps for the pipeline, and any data references that need to passed between them. In this case, the first step must write the prepared data to a folder that can be read from by the second step. Since the steps will be run on remote compute (and in fact, could each be run on different compute), the folder path must be passed as a data reference to a location in a datastore within the workspace. The PipelineData object is a special kind of data reference that is used for interim storage locations that can be passed between pipeline steps, so you'll create one and use at as the output for the first step and the input for the second step. Note that you also need to pass it as a script argument so our code can access the datastore location referenced by the data reference."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "tags": [
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline steps defined\n",
            "Pipeline is built.\n",
            "Created step Prepare Data [19fe99c8][b8e437b4-fe08-4c72-8efd-0ea5254e1ecf], (This step will run and generate new outputs)\n",
            "Created step Train and Register Model [6a8d6279][09a9e26c-fd9a-4003-b7ec-75b210b73ef7], (This step will run and generate new outputs)\n",
            "Submitted PipelineRun f0eb4c10-cfbc-414d-8a90-5bc61eb511fc\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/f0eb4c10-cfbc-414d-8a90-5bc61eb511fc?wsid=/subscriptions/d87c2610-2653-4ec8-aa6b-1aa1edd5ec61/resourcegroups/poc_aml/workspaces/isolation_forest&tid=4328c8ea-c698-4ade-a183-4a9f640776aa\n",
            "Pipeline submitted for execution.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7cfbe95090743128bc0b173687f1513"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/aml.mini.widget.v1": "{\"status\": \"Completed\", \"workbench_run_details_uri\": \"https://ml.azure.com/runs/f0eb4c10-cfbc-414d-8a90-5bc61eb511fc?wsid=/subscriptions/d87c2610-2653-4ec8-aa6b-1aa1edd5ec61/resourcegroups/poc_aml/workspaces/isolation_forest&tid=4328c8ea-c698-4ade-a183-4a9f640776aa\", \"run_id\": \"f0eb4c10-cfbc-414d-8a90-5bc61eb511fc\", \"run_properties\": {\"run_id\": \"f0eb4c10-cfbc-414d-8a90-5bc61eb511fc\", \"created_utc\": \"2021-04-29T11:50:43.919229Z\", \"properties\": {\"azureml.runsource\": \"azureml.PipelineRun\", \"runSource\": \"SDK\", \"runType\": \"SDK\", \"azureml.parameters\": \"{}\"}, \"tags\": {\"azureml.pipelineComponent\": \"pipelinerun\"}, \"end_time_utc\": \"2021-04-29T11:58:23.673696Z\", \"status\": \"Completed\", \"log_files\": {\"logs/azureml/executionlogs.txt\": \"https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.f0eb4c10-cfbc-414d-8a90-5bc61eb511fc/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=yyE5u4vTEH0digPvKwTLqd8UpoK4V7LYUwhwgf%2BidNQ%3D&st=2021-04-29T12%3A41%3A19Z&se=2021-04-29T20%3A51%3A19Z&sp=r\", \"logs/azureml/stderrlogs.txt\": \"https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.f0eb4c10-cfbc-414d-8a90-5bc61eb511fc/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=mPYe4QxO6XdshRenJGS8oGocgBWdYrr5VYkIM5Qynr8%3D&st=2021-04-29T12%3A41%3A19Z&se=2021-04-29T20%3A51%3A19Z&sp=r\", \"logs/azureml/stdoutlogs.txt\": \"https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.f0eb4c10-cfbc-414d-8a90-5bc61eb511fc/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=yv4fHF7t1kDfWM818DqylxRum5KCGKkOMpBSwjxShGA%3D&st=2021-04-29T12%3A41%3A19Z&se=2021-04-29T20%3A51%3A19Z&sp=r\"}, \"log_groups\": [[\"logs/azureml/executionlogs.txt\", \"logs/azureml/stderrlogs.txt\", \"logs/azureml/stdoutlogs.txt\"]], \"run_duration\": \"0:07:39\", \"run_number\": \"13\", \"run_queued_details\": {\"status\": \"Finished\", \"details\": null}}, \"child_runs\": [{\"run_id\": \"2f556e67-2a47-4eb7-9006-f46cdd9b909b\", \"name\": \"Prepare Data\", \"status\": \"Finished\", \"start_time\": \"2021-04-29T11:54:41.237372Z\", \"created_time\": \"2021-04-29T11:50:51.519649Z\", \"end_time\": \"2021-04-29T11:56:50.209013Z\", \"duration\": \"0:05:58\", \"run_number\": 14, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-04-29T11:50:51.519649Z\", \"is_reused\": \"\"}, {\"run_id\": \"37463da7-5d0f-401f-aad0-823908b92e96\", \"name\": \"Train and Register Model\", \"status\": \"Finished\", \"start_time\": \"2021-04-29T11:57:04.665528Z\", \"created_time\": \"2021-04-29T11:56:53.208189Z\", \"end_time\": \"2021-04-29T11:58:20.660654Z\", \"duration\": \"0:01:27\", \"run_number\": 15, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-04-29T11:56:53.208189Z\", \"is_reused\": \"\"}], \"children_metrics\": {\"categories\": null, \"series\": null, \"metricName\": null}, \"run_metrics\": [], \"run_logs\": \"[2021-04-29 11:50:51Z] Submitting 1 runs, first five are: 19fe99c8:2f556e67-2a47-4eb7-9006-f46cdd9b909b\\n[2021-04-29 11:56:52Z] Completing processing run id 2f556e67-2a47-4eb7-9006-f46cdd9b909b.\\n[2021-04-29 11:56:53Z] Submitting 1 runs, first five are: 6a8d6279:37463da7-5d0f-401f-aad0-823908b92e96\\n[2021-04-29 11:58:23Z] Completing processing run id 37463da7-5d0f-401f-aad0-823908b92e96.\\n\\nRun is completed.\", \"graph\": {\"datasource_nodes\": {\"660f915f\": {\"node_id\": \"660f915f\", \"name\": \"diabetes dataset\"}}, \"module_nodes\": {\"19fe99c8\": {\"node_id\": \"19fe99c8\", \"name\": \"Prepare Data\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"2f556e67-2a47-4eb7-9006-f46cdd9b909b\"}, \"6a8d6279\": {\"node_id\": \"6a8d6279\", \"name\": \"Train and Register Model\", \"status\": \"Finished\", \"_is_reused\": false, \"run_id\": \"37463da7-5d0f-401f-aad0-823908b92e96\"}}, \"edges\": [{\"source_node_id\": \"660f915f\", \"source_node_name\": \"diabetes dataset\", \"source_name\": \"data\", \"target_name\": \"raw_data\", \"dst_node_id\": \"19fe99c8\", \"dst_node_name\": \"Prepare Data\"}, {\"source_node_id\": \"19fe99c8\", \"source_node_name\": \"Prepare Data\", \"source_name\": \"prepped_data_folder\", \"target_name\": \"prepped_data_folder\", \"dst_node_id\": \"6a8d6279\", \"dst_node_name\": \"Train and Register Model\"}], \"child_runs\": [{\"run_id\": \"2f556e67-2a47-4eb7-9006-f46cdd9b909b\", \"name\": \"Prepare Data\", \"status\": \"Finished\", \"start_time\": \"2021-04-29T11:54:41.237372Z\", \"created_time\": \"2021-04-29T11:50:51.519649Z\", \"end_time\": \"2021-04-29T11:56:50.209013Z\", \"duration\": \"0:05:58\", \"run_number\": 14, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-04-29T11:50:51.519649Z\", \"is_reused\": \"\"}, {\"run_id\": \"37463da7-5d0f-401f-aad0-823908b92e96\", \"name\": \"Train and Register Model\", \"status\": \"Finished\", \"start_time\": \"2021-04-29T11:57:04.665528Z\", \"created_time\": \"2021-04-29T11:56:53.208189Z\", \"end_time\": \"2021-04-29T11:58:20.660654Z\", \"duration\": \"0:01:27\", \"run_number\": 15, \"metric\": null, \"run_type\": \"azureml.StepRun\", \"training_percent\": null, \"created_time_dt\": \"2021-04-29T11:56:53.208189Z\", \"is_reused\": \"\"}]}, \"widget_settings\": {\"childWidgetDisplay\": \"popup\", \"send_telemetry\": false, \"log_level\": \"INFO\", \"sdk_version\": \"1.27.0\"}, \"loading\": false}"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/dataprep/backgroundProcess.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/dataprep/backgroundProcess.log?sv=2019-02-02&sr=b&sig=Gru%2BqfeP4%2BotThNE0JwrUuz%2BDIlwBWPQfQroguHQN%2Fo%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/dataprep/backgroundProcess_Telemetry.log?sv=2019-02-02&sr=b&sig=3YCmxZTT6sy%2BjVsmJOyDwm%2FSLVwxXcjoGfqfYE5W55w%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=%2BMeunUvuCjb8G5WtxWNal0bciQGUyqDr86zjvRUEMqM%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=ZmOcHCIM1MF9LrXJGL%2BOc4U6bziUIyXkpYMy4xqgKo0%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=zXBbmGZQQPvCjHxb3TfVDdWjk1RvP0NYElE9qd3D2bk%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=dKM168joDvL5cKMDgZDJ4MufWQ8hR6nLEypZui24ieY%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.2f556e67-2a47-4eb7-9006-f46cdd9b909b/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=tzaLOLL3ugTEPKuPg0OdxrhyBW1rVwd54npqORMnihk%3D&st=2021-04-29T11%3A46%3A43Z&se=2021-04-29T19%3A56%3A43Z&sp=r'}, 'submittedBy': 'Walid Tokhi'}\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "StepRunId: 37463da7-5d0f-401f-aad0-823908b92e96\n",
            "Link to Azure Machine Learning Portal: https://ml.azure.com/runs/37463da7-5d0f-401f-aad0-823908b92e96?wsid=/subscriptions/d87c2610-2653-4ec8-aa6b-1aa1edd5ec61/resourcegroups/poc_aml/workspaces/isolation_forest&tid=4328c8ea-c698-4ade-a183-4a9f640776aa\n",
            "StepRun( Train and Register Model ) Status: NotStarted\n",
            "StepRun( Train and Register Model ) Status: Running\n",
            "\n",
            "Streaming azureml-logs/55_azureml-execution-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt\n",
            "========================================================================================================================\n",
            "2021-04-29T11:57:04Z Successfully mounted a/an Blobfuse File System at /mnt/batch/tasks/shared/LS_root/jobs/isolation_forest/azureml/37463da7-5d0f-401f-aad0-823908b92e96/mounts/workspaceblobstore\n",
            "2021-04-29T11:57:05Z Starting output-watcher...\n",
            "2021-04-29T11:57:05Z IsDedicatedCompute == True, won't poll for Low Pri Preemption\n",
            "2021-04-29T11:57:06Z Executing 'Copy ACR Details file' on 10.0.0.4\n",
            "2021-04-29T11:57:06Z Copy ACR Details file succeeded on 10.0.0.4. Output: \n",
            ">>>   \n",
            ">>>   \n",
            "Login Succeeded\n",
            "Using default tag: latest\n",
            "latest: Pulling from azureml/azureml_443e6f417ae4fd3295e21acd32219350\n",
            "4007a89234b4: Pulling fs layer\n",
            "5dfa26c6b9c9: Pulling fs layer\n",
            "0ba7bf18aa40: Pulling fs layer\n",
            "4c6ec688ebe3: Pulling fs layer\n",
            "574f361512d6: Pulling fs layer\n",
            "db4d1e2d7079: Pulling fs layer\n",
            "e544ee0f522d: Pulling fs layer\n",
            "c655136086be: Pulling fs layer\n",
            "2ec37f44090c: Pulling fs layer\n",
            "5fba3bd4a2c4: Pulling fs layer\n",
            "7e0ea9d0a1ab: Pulling fs layer\n",
            "da005f826951: Pulling fs layer\n",
            "c69668c20bf1: Pulling fs layer\n",
            "952e2bcd6296: Pulling fs layer\n",
            "0e10f56a35da: Pulling fs layer\n",
            "dd30ce6fbf17: Pulling fs layer\n",
            "140dcfe296da: Pulling fs layer\n",
            "1e0ddf59df63: Pulling fs layer\n",
            "74d77bb5498c: Pulling fs layer\n",
            "c78b8f5866be: Pulling fs layer\n",
            "0ab763825413: Pulling fs layer\n",
            "da005f826951: Waiting\n",
            "c69668c20bf1: Waiting\n",
            "952e2bcd6296: Waiting\n",
            "0e10f56a35da: Waiting\n",
            "dd30ce6fbf17: Waiting\n",
            "140dcfe296da: Waiting\n",
            "1e0ddf59df63: Waiting\n",
            "74d77bb5498c: Waiting\n",
            "c78b8f5866be: Waiting\n",
            "0ab763825413: Waiting\n",
            "4c6ec688ebe3: Waiting\n",
            "574f361512d6: Waiting\n",
            "db4d1e2d7079: Waiting\n",
            "e544ee0f522d: Waiting\n",
            "c655136086be: Waiting\n",
            "2ec37f44090c: Waiting\n",
            "7e0ea9d0a1ab: Waiting\n",
            "5fba3bd4a2c4: Waiting\n",
            "5dfa26c6b9c9: Verifying Checksum\n",
            "5dfa26c6b9c9: Download complete\n",
            "0ba7bf18aa40: Verifying Checksum\n",
            "0ba7bf18aa40: Download complete\n",
            "4c6ec688ebe3: Verifying Checksum\n",
            "4c6ec688ebe3: Download complete\n",
            "4007a89234b4: Verifying Checksum\n",
            "4007a89234b4: Download complete\n",
            "db4d1e2d7079: Verifying Checksum\n",
            "db4d1e2d7079: Download complete\n",
            "574f361512d6: Verifying Checksum\n",
            "574f361512d6: Download complete\n",
            "e544ee0f522d: Verifying Checksum\n",
            "e544ee0f522d: Download complete\n",
            "5fba3bd4a2c4: Verifying Checksum\n",
            "5fba3bd4a2c4: Download complete\n",
            "2ec37f44090c: Verifying Checksum\n",
            "2ec37f44090c: Download complete\n",
            "c655136086be: Verifying Checksum\n",
            "c655136086be: Download complete\n",
            "7e0ea9d0a1ab: Verifying Checksum\n",
            "7e0ea9d0a1ab: Download complete\n",
            "da005f826951: Verifying Checksum\n",
            "da005f826951: Download complete\n",
            "c69668c20bf1: Verifying Checksum\n",
            "c69668c20bf1: Download complete\n",
            "952e2bcd6296: Verifying Checksum\n",
            "952e2bcd6296: Download complete\n",
            "dd30ce6fbf17: Verifying Checksum\n",
            "dd30ce6fbf17: Download complete\n",
            "0e10f56a35da: Verifying Checksum\n",
            "0e10f56a35da: Download complete\n",
            "74d77bb5498c: Verifying Checksum\n",
            "74d77bb5498c: Download complete\n",
            "1e0ddf59df63: Verifying Checksum\n",
            "1e0ddf59df63: Download complete\n",
            "0ab763825413: Verifying Checksum\n",
            "0ab763825413: Download complete\n",
            "c78b8f5866be: Verifying Checksum\n",
            "c78b8f5866be: Download complete\n",
            "4007a89234b4: Pull complete\n",
            "5dfa26c6b9c9: Pull complete\n",
            "0ba7bf18aa40: Pull complete\n",
            "4c6ec688ebe3: Pull complete\n",
            "574f361512d6: Pull complete\n",
            "140dcfe296da: Verifying Checksum\n",
            "140dcfe296da: Download complete\n",
            "db4d1e2d7079: Pull complete\n",
            "e544ee0f522d: Pull complete\n",
            "c655136086be: Pull complete\n",
            "2ec37f44090c: Pull complete\n",
            "5fba3bd4a2c4: Pull complete\n",
            "7e0ea9d0a1ab: Pull complete\n",
            "da005f826951: Pull complete\n",
            "c69668c20bf1: Pull complete\n",
            "952e2bcd6296: Pull complete\n",
            "0e10f56a35da: Pull complete\n",
            "dd30ce6fbf17: Pull complete\n",
            "140dcfe296da: Pull complete\n",
            "1e0ddf59df63: Pull complete\n",
            "74d77bb5498c: Pull complete\n",
            "c78b8f5866be: Pull complete\n",
            "0ab763825413: Pull complete\n",
            "Digest: sha256:a729479a8921c6ab99186b14bcd5d7d416dd16bde40e7898890a5c537df940e5\n",
            "Status: Downloaded newer image for bccac40ca8fe45fe9133379c6ac3ede7.azurecr.io/azureml/azureml_443e6f417ae4fd3295e21acd32219350:latest\n",
            "bccac40ca8fe45fe9133379c6ac3ede7.azurecr.io/azureml/azureml_443e6f417ae4fd3295e21acd32219350:latest\n",
            "2021-04-29T11:57:49Z Check if container 37463da7-5d0f-401f-aad0-823908b92e96 already exist exited with 0, \n",
            "\n",
            "\n",
            "Streaming azureml-logs/70_driver_log.txt\n",
            "========================================\n",
            "2021/04/29 11:58:02 Starting App Insight Logger for task:  runTaskLet\n",
            "2021/04/29 11:58:02 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/info\n",
            "2021/04/29 11:58:02 Attempt 1 of http call to http://10.0.0.4:16384/sendlogstoartifacts/status\n",
            "[2021-04-29T11:58:02.650874] Entering context manager injector.\n",
            "[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_diabetes.py', '--training-folder', '/mnt/batch/tasks/shared/LS_root/jobs/isolation_forest/azureml/37463da7-5d0f-401f-aad0-823908b92e96/mounts/workspaceblobstore/azureml/2f556e67-2a47-4eb7-9006-f46cdd9b909b/prepped_data_folder'])\n",
            "Script type = None\n",
            "[2021-04-29T11:58:04.077194] Entering Run History Context Manager.\n",
            "[2021-04-29T11:58:04.720778] Current directory: /mnt/batch/tasks/shared/LS_root/jobs/isolation_forest/azureml/37463da7-5d0f-401f-aad0-823908b92e96/mounts/workspaceblobstore/azureml/37463da7-5d0f-401f-aad0-823908b92e96\n",
            "[2021-04-29T11:58:04.721067] Preparing to call script [train_diabetes.py] with arguments:['--training-folder', '/mnt/batch/tasks/shared/LS_root/jobs/isolation_forest/azureml/37463da7-5d0f-401f-aad0-823908b92e96/mounts/workspaceblobstore/azureml/2f556e67-2a47-4eb7-9006-f46cdd9b909b/prepped_data_folder']\n",
            "[2021-04-29T11:58:04.721113] After variable expansion, calling script [train_diabetes.py] with arguments:['--training-folder', '/mnt/batch/tasks/shared/LS_root/jobs/isolation_forest/azureml/37463da7-5d0f-401f-aad0-823908b92e96/mounts/workspaceblobstore/azureml/2f556e67-2a47-4eb7-9006-f46cdd9b909b/prepped_data_folder']\n",
            "\n",
            "\n",
            "Streaming azureml-logs/75_job_post-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt\n",
            "===============================================================================================================\n",
            "[2021-04-29T11:58:11.710023] Entering job release\n",
            "[2021-04-29T11:58:12.774773] Starting job release\n",
            "[2021-04-29T11:58:12.780035] Logging experiment finalizing status in history service.\n",
            "[2021-04-29T11:58:12.780349] job release stage : upload_datastore starting...\n",
            "Starting the daemon thread to refresh tokens in background for process with pid = 148\n",
            "[2021-04-29T11:58:12.780734] job release stage : start importing azureml.history._tracking in run_history_release.\n",
            "[2021-04-29T11:58:12.780777] job release stage : execute_job_release starting...\n",
            "[2021-04-29T11:58:12.781410] job release stage : copy_batchai_cached_logs starting...\n",
            "[2021-04-29T11:58:12.790936] job release stage : copy_batchai_cached_logs completed...\n",
            "[2021-04-29T11:58:12.792481] Entering context manager injector.\n",
            "[2021-04-29T11:58:12.816550] job release stage : upload_datastore completed...\n",
            "[2021-04-29T11:58:12.953860] job release stage : execute_job_release completed...\n",
            "[2021-04-29T11:58:12.979061] job release stage : send_run_telemetry starting...\n",
            "[2021-04-29T11:58:13.236636] get vm size and vm region successfully.\n",
            "[2021-04-29T11:58:13.468674] get compute meta data successfully.\n",
            "[2021-04-29T11:58:13.687777] post artifact meta request successfully.\n",
            "[2021-04-29T11:58:13.735996] upload compute record artifact successfully.\n",
            "[2021-04-29T11:58:13.736184] job release stage : send_run_telemetry completed...\n",
            "[2021-04-29T11:58:13.736519] Job release is complete\n",
            "\n",
            "StepRun(Train and Register Model) Execution Summary\n",
            "====================================================\n",
            "StepRun( Train and Register Model ) Status: Finished\n",
            "{'runId': '37463da7-5d0f-401f-aad0-823908b92e96', 'target': 'regression', 'status': 'Completed', 'startTimeUtc': '2021-04-29T11:57:04.665528Z', 'endTimeUtc': '2021-04-29T11:58:20.660654Z', 'properties': {'ContentSnapshotId': '3ce4597c-3c69-45e9-9065-2692a87b7b22', 'StepType': 'PythonScriptStep', 'ComputeTargetType': 'AmlCompute', 'azureml.moduleid': '09a9e26c-fd9a-4003-b7ec-75b210b73ef7', 'azureml.runsource': 'azureml.StepRun', 'azureml.nodeid': '6a8d6279', 'azureml.pipelinerunid': 'f0eb4c10-cfbc-414d-8a90-5bc61eb511fc', '_azureml.ComputeTargetType': 'amlcompute', 'ProcessInfoFile': 'azureml-logs/process_info.json', 'ProcessStatusFile': 'azureml-logs/process_status.json'}, 'inputDatasets': [], 'outputDatasets': [], 'runDefinition': {'script': 'train_diabetes.py', 'command': '', 'useAbsolutePath': False, 'arguments': ['--training-folder', '$AZUREML_DATAREFERENCE_prepped_data_folder'], 'sourceDirectoryDataStore': None, 'framework': 'Python', 'communicator': 'None', 'target': 'regression', 'dataReferences': {'prepped_data_folder': {'dataStoreName': 'workspaceblobstore', 'mode': 'Mount', 'pathOnDataStore': 'azureml/2f556e67-2a47-4eb7-9006-f46cdd9b909b/prepped_data_folder', 'pathOnCompute': None, 'overwrite': False}}, 'data': {}, 'outputData': {}, 'jobName': None, 'maxRunDurationSeconds': None, 'nodeCount': 1, 'priority': None, 'credentialPassthrough': False, 'identity': None, 'environment': {'name': 'diabetes-pipeline-env', 'version': '1', 'python': {'interpreterPath': 'python', 'userManagedDependencies': False, 'condaDependencies': {'channels': ['anaconda', 'conda-forge'], 'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults~=1.27.0', 'azureml-dataprep[pandas]', 'pyarrow']}, 'scikit-learn', 'ipykernel', 'matplotlib', 'pandas', 'pip'], 'name': 'azureml_28b0d742a4cb5c02a5befb218632cd1a'}, 'baseCondaEnvironment': None}, 'environmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'}, 'docker': {'baseImage': 'mcr.microsoft.com/azureml/intelmpi2018.3-ubuntu16.04:20210301.v1', 'platform': {'os': 'Linux', 'architecture': 'amd64'}, 'baseDockerfile': None, 'baseImageRegistry': {'address': None, 'username': None, 'password': None}, 'enabled': True, 'arguments': []}, 'spark': {'repositories': [], 'packages': [], 'precachePackages': True}, 'inferencingStackVersion': None}, 'history': {'outputCollection': True, 'directoriesToWatch': ['logs'], 'enableMLflowTracking': True, 'snapshotProject': True}, 'spark': {'configuration': {'spark.app.name': 'Azure ML Experiment', 'spark.yarn.maxAppAttempts': '1'}}, 'parallelTask': {'maxRetriesPerWorker': 0, 'workerCountPerNode': 1, 'terminalExitCodes': None, 'configuration': {}}, 'amlCompute': {'name': None, 'vmSize': None, 'retainCluster': False, 'clusterMaxNodeCount': 1}, 'aiSuperComputer': {'instanceType': None, 'imageVersion': None, 'location': None, 'aiSuperComputerStorageData': None, 'interactive': False, 'scalePolicy': None, 'virtualClusterArmId': None, 'tensorboardLogDirectory': None, 'sshPublicKey': None}, 'tensorflow': {'workerCount': 1, 'parameterServerCount': 1}, 'mpi': {'processCountPerNode': 1}, 'pyTorch': {'communicationBackend': 'nccl', 'processCount': None}, 'hdi': {'yarnDeployMode': 'Cluster'}, 'containerInstance': {'region': None, 'cpuCores': 2.0, 'memoryGb': 3.5}, 'exposedPorts': None, 'docker': {'useDocker': False, 'sharedVolumes': True, 'shmSize': '2g', 'arguments': []}, 'cmk8sCompute': {'configuration': {}}, 'commandReturnCodeConfig': {'returnCode': 'Zero', 'successfulReturnCodes': []}, 'environmentVariables': {}, 'applicationEndpoints': {}}, 'logFiles': {'azureml-logs/55_azureml-execution-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/azureml-logs/55_azureml-execution-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt?sv=2019-02-02&sr=b&sig=4ivl6YVbiYcpR7E7QH%2BHkiCJD6Sv2oxYSeMlOhnQ2Wg%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'azureml-logs/65_job_prep-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/azureml-logs/65_job_prep-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt?sv=2019-02-02&sr=b&sig=%2ByzjbacubUx1u8j8kHx6vb1bL1CboZdIZ4NyCCUxLtI%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'azureml-logs/70_driver_log.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/azureml-logs/70_driver_log.txt?sv=2019-02-02&sr=b&sig=rtO18zkIOOvxDhTOBFDTLf3qD6FHworbLofLXNHcD70%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'azureml-logs/75_job_post-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/azureml-logs/75_job_post-tvmps_fa1194deb2d3cc8c61706cae4079f2217a233cbcd05671a2cfdeedc479fd68b7_d.txt?sv=2019-02-02&sr=b&sig=jWqEgAYpmBgsrObqSG1h3n9yWq0dHsZL6hkEWvsCLlY%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'azureml-logs/process_info.json': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/azureml-logs/process_info.json?sv=2019-02-02&sr=b&sig=ro9p8bkESzFCqFs9ktoeJZYR9zWN%2FU%2FheChFkPNmQlo%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'azureml-logs/process_status.json': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/azureml-logs/process_status.json?sv=2019-02-02&sr=b&sig=GPPXDT7eFEYR24QSlxwi48Dec63gn5okRsy4euU3cgM%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'logs/azureml/106_azureml.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/logs/azureml/106_azureml.log?sv=2019-02-02&sr=b&sig=Jz8Zl3Jw7kF%2BQ%2BVV1nxHklxw3tsn5a6dWeqORe%2FMVJU%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'logs/azureml/executionlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=d6ZlFq3Cx196XMZSbH1iVqSUDksmV8pShqj2%2Bq4Xg6k%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'logs/azureml/job_prep_azureml.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/logs/azureml/job_prep_azureml.log?sv=2019-02-02&sr=b&sig=%2BhhuM4cXkWPxj0WOCsMXG5A%2B2RjkWESBu7j%2Fa9ttxfc%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'logs/azureml/job_release_azureml.log': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/logs/azureml/job_release_azureml.log?sv=2019-02-02&sr=b&sig=HVlPaZFbI82LeJbgs9iWaARKhPAzECFCz58tR63fFRw%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=RwjMYU36tY67l4b4Db%2F0GBVLWiS5YsOpJ3pHr7yc4H4%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.37463da7-5d0f-401f-aad0-823908b92e96/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=g0XDN9QKPIfNB3PRyBaFY7RWeBhlrfTfUWXVrHQhDI4%3D&st=2021-04-29T11%3A48%3A15Z&se=2021-04-29T19%3A58%3A15Z&sp=r'}, 'submittedBy': 'Walid Tokhi'}\n",
            "\n",
            "\n",
            "\n",
            "PipelineRun Execution Summary\n",
            "==============================\n",
            "PipelineRun Status: Finished\n",
            "{'runId': 'f0eb4c10-cfbc-414d-8a90-5bc61eb511fc', 'status': 'Completed', 'startTimeUtc': '2021-04-29T11:50:45.358106Z', 'endTimeUtc': '2021-04-29T11:58:23.673696Z', 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'SDK', 'runType': 'SDK', 'azureml.parameters': '{}'}, 'inputDatasets': [], 'outputDatasets': [], 'logFiles': {'logs/azureml/executionlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.f0eb4c10-cfbc-414d-8a90-5bc61eb511fc/logs/azureml/executionlogs.txt?sv=2019-02-02&sr=b&sig=LaHIExyq%2FhaH5eU7j%2BoDXFRSiY%2BLB8rffV6HQTSv3ak%3D&st=2021-04-29T11%3A41%3A07Z&se=2021-04-29T19%3A51%3A07Z&sp=r', 'logs/azureml/stderrlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.f0eb4c10-cfbc-414d-8a90-5bc61eb511fc/logs/azureml/stderrlogs.txt?sv=2019-02-02&sr=b&sig=9jlSTdWrmD%2F%2FgkgOos7xaFtxKPF1wNWajsLqLSTkiV0%3D&st=2021-04-29T11%3A41%3A07Z&se=2021-04-29T19%3A51%3A07Z&sp=r', 'logs/azureml/stdoutlogs.txt': 'https://isolationfores1169513392.blob.core.windows.net/azureml/ExperimentRun/dcid.f0eb4c10-cfbc-414d-8a90-5bc61eb511fc/logs/azureml/stdoutlogs.txt?sv=2019-02-02&sr=b&sig=ogs54zsclPfS3Z4jlGDNQXJ%2BrWwe7LqtKv89Whp%2FFUg%3D&st=2021-04-29T11%3A41%3A07Z&se=2021-04-29T19%3A51%3A07Z&sp=r'}, 'submittedBy': 'Walid Tokhi'}\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Finished'"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from azureml.pipeline.core import PipelineData\n",
        "from azureml.pipeline.steps import PythonScriptStep\n",
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import Pipeline\n",
        "from azureml.widgets import RunDetails\n",
        "\n",
        "# Get the training dataset\n",
        "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
        "\n",
        "# Create a PipelineData (temporary Data Reference) for the model folder\n",
        "prepped_data_folder = PipelineData(\"prepped_data_folder\", datastore=ws.get_default_datastore())\n",
        "\n",
        "# Step 1, Run the data prep script\n",
        "prep_step = PythonScriptStep(name = \"Prepare Data\",\n",
        "                                source_directory = experiment_folder,\n",
        "                                script_name = \"prep_diabetes.py\",\n",
        "                                arguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n",
        "                                             '--prepped-data', prepped_data_folder],\n",
        "                                outputs=[prepped_data_folder],\n",
        "                                compute_target = pipeline_cluster,\n",
        "                                runconfig = pipeline_run_config,\n",
        "                                allow_reuse = True)\n",
        "\n",
        "# Step 2, run the training script\n",
        "train_step = PythonScriptStep(name = \"Train and Register Model\",\n",
        "                                source_directory = experiment_folder,\n",
        "                                script_name = \"train_diabetes.py\",\n",
        "                                arguments = ['--training-folder', prepped_data_folder],\n",
        "                                inputs=[prepped_data_folder],\n",
        "                                compute_target = pipeline_cluster,\n",
        "                                runconfig = pipeline_run_config,\n",
        "                                allow_reuse = True)\n",
        "\n",
        "print(\"Pipeline steps defined\")\n",
        "\n",
        "#OK, you're ready build the pipeline from the steps you've defined and run it as an experiment.\n",
        "# Construct the pipeline\n",
        "pipeline = Pipeline(workspace=ws, steps=[prep_step, train_step])\n",
        "print(\"Pipeline is built.\")\n",
        "\n",
        "# Create an experiment and run the pipeline\n",
        "experiment = Experiment(workspace=ws, name = 'mslearn-diabetes-pipeline')\n",
        "pipeline_run = experiment.submit(pipeline, regenerate_outputs=True)\n",
        "print(\"Pipeline submitted for execution.\")\n",
        "RunDetails(pipeline_run).show()\n",
        "pipeline_run.wait_for_completion(show_output=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train and Register Model :\n",
            "\t Accuracy : 0.8896666666666667\n",
            "\t AUC : 0.8783148415344046\n",
            "\t ROC : aml://artifactId/ExperimentRun/dcid.d0241506-67f9-4b9c-9aff-6357a1bd0516/ROC_1619164386.png\n",
            "Prepare Data :\n",
            "\t raw_rows : 10000\n",
            "\t processed_rows : 10000\n"
          ]
        }
      ],
      "source": [
        "for run in pipeline_run.get_children():\n",
        "    print(run.name, ':')\n",
        "    metrics = run.get_metrics()\n",
        "    for metric_name in metrics:\n",
        "        print('\\t',metric_name, \":\", metrics[metric_name])"
      ]
    },
    {
      "source": [
        "Assuming the pipeline was successful, a new model should be registered with a Training context tag indicating it was trained in a pipeline. Run the following code to verify this.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "diabetes_model version: 4\n\t Training context : Pipeline\n\t AUC : 0.8783148415344046\n\t Accuracy : 0.8896666666666667\n\n\ndiabetes_model version: 3\n\t Training context : Inline Training\n\t AUC : 0.8785884869894024\n\t Accuracy : 0.891\n\n\ndiabetes_model version: 2\n\t Training context : Inline Training\n\t AUC : 0.8793347927757599\n\t Accuracy : 0.8916666666666667\n\n\ndiabetes_model version: 1\n\t Training context : Inline Training\n\t AUC : 0.8760908502910593\n\t Accuracy : 0.8883333333333333\n\n\nsalary_model version: 1\n\t version : 1\n\n\niforest_script version: 4\n\t version : 1\n\n\niforest_script version: 3\n\t version : 1\n\n\niforest_script version: 2\n\t version : 1\n\n\niforest_script version: 1\n\t version : 1\n\n\nmodel version: 7\n\t version : 1\n\n\nmodel version: 6\n\t version : 1\n\n\niforest_salary_model version: 1\n\t version : 1\n\n\nmodel version: 5\n\t version : 1\n\n\nmodel version: 4\n\t version : 1\n\n\nmodel version: 3\n\t version : 1\n\n\nmodel version: 2\n\t version : 1\n\n\nmodel version: 1\n\t version : 1\n\n\nisolation-forest version: 1\n\t  : \n\n\n"
          ]
        }
      ],
      "source": [
        "from azureml.core import Model\n",
        "\n",
        "for model in Model.list(ws):\n",
        "    print(model.name, 'version:', model.version)\n",
        "    for tag_name in model.tags:\n",
        "        tag = model.tags[tag_name]\n",
        "        print ('\\t',tag_name, ':', tag)\n",
        "    for prop_name in model.properties:\n",
        "        prop = model.properties[prop_name]\n",
        "        print ('\\t',prop_name, ':', prop)\n",
        "    print('\\n')"
      ]
    },
    {
      "source": [
        "## Publish the pipeline\n",
        "After you've created and tested a pipeline, you can publish it as a REST service.\n",
        "\n"
      ],
      "cell_type": "markdown",
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python385jvsc74a57bd0699e09c9557adb08d12eef13f3a3e1a8c979f64b2e90507ef11fc0360b4180e2",
      "display_name": "Python 3.8.5 64-bit ('base': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}